{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "##    Description    Functions to manage SDFiles, pandas Dataframes ...\n",
    "##                   Applicability Domain analysis\n",
    "##                   \n",
    "##    Authors:       Kevin Pinto Gil (kevin.pinto@upf.edu)\n",
    "##                   Manuel Pastor (manuel.pastor@upf.edu)\n",
    "##\n",
    "##    Copyright 2018 Manuel Pastor\n",
    "##\n",
    "##    This file is part of PhiTools\n",
    "##\n",
    "##    PhiTools is free software: you can redistribute it and/or modify\n",
    "##    it under the terms of the GNU General Public License as published by\n",
    "##    the Free Software Foundation version 3.\n",
    "##\n",
    "##    PhiTools is distributed in the hope that it will be useful,\n",
    "##    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "##    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "##    GNU General Public License for more details.\n",
    "##\n",
    "##    You should have received a copy of the GNU General Public License\n",
    "##    along with PhiTools.  If not, see <http://www.gnu.org/licenses/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Could not find EPA module. Will use only the CACTVS web service to resolve CAS number structures. ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### System libraries\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import getopt\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "### General libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import * #math commands will be available every time you start an interactive session\n",
    "\n",
    "## RDkit libraries\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, PandasTools, AllChem, Descriptors, Crippen, DataStructs\n",
    "\n",
    "### Scikit learn libraries\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Dataframe visualization part\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.max_rows = 4000\n",
    "\n",
    "## Ignore Warnings \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Calculating Morgan FingerPrints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_fp_arr( mols, radius ):\n",
    "\n",
    "    '''\n",
    "    Info\n",
    "    ----\n",
    "    This function allows one to calculate morgan fingerprints from a list of molecules\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    radius: int\n",
    "        e.g. radius = 4 ## define morgan fingerprint radius \n",
    "    mols: RDKit molecule\n",
    "        e.g. mols = df.mol3DProt ## dataframe column containing molecules\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    morgan fingerprints array\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    \n",
    "    res = calc_fp_arr( mols , radius)\n",
    "    \n",
    "    Then one can run a PCA such as:\n",
    "    \n",
    "        pca = PCA( n_components = 3 )\n",
    "        pca.fit( res )\n",
    "        x = pca.transform( res )\n",
    "        x = pd.DataFrame( x )\n",
    "        x.columns = [ 'PC1', 'PC2', 'PC3' ]\n",
    "        df = df.join(x)\n",
    "\n",
    "    Attention\n",
    "    ---------\n",
    "    \n",
    "    If this function doesn't work, try to update your conda environment\n",
    "    \n",
    "    '''\n",
    "\n",
    "    fplist = []\n",
    "    for mol in mols:\n",
    "        arr = np.zeros( (1,) )\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect( mol, int(radius))\n",
    "        DataStructs.ConvertToNumpyArray( fp, arr )\n",
    "        fplist.append( arr )\n",
    "    return np.asarray( fplist )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Running  PCA using external Descriptors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPCAscores(descriptors, n_components):\n",
    "\n",
    "    '''\n",
    "\n",
    "    This functions returns as an output a PCA pandas Dataframe with the number\n",
    "    of components calculated \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    descriptors: numpy array or pd.series pandas dataframe columns\n",
    "        Numpy array with descriptors e.g. descriptors = [[0,0,1],[...],[1,0,1]]\n",
    "        or Pandas DataFrame columns  e.g. descriptors = df_piv.iloc[:,2:]\n",
    "    \n",
    "    n_components: int\n",
    "        ## number of components one wants to calculate\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    pd.DataFrame\n",
    "        Principal Component Analysis (PCA) DataFrame\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    \n",
    "    descriptors = df_piv.iloc[:,2:]\n",
    "    n_components =  3\n",
    "    \n",
    "    getPCAscores(descriptors, n_components)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    pca = PCA( n_components = n_components )\n",
    "    pca.fit( descriptors )\n",
    "    pcaDF = pca.transform( descriptors )\n",
    "    pcaDF = pd.DataFrame( pcaDF )\n",
    "    pcaDF.columns = [ 'PC'+str(i+1) for i in range(n_components) ]\n",
    "    \n",
    "    return pcaDF\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Similarity Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Similarity between two Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMorganfpDF(inSDF, radius):\n",
    "   \n",
    "    '''\n",
    "        Getting morgan fingerprints from SDFile. The output will be a dataframe including\n",
    "        in one column the morgan Fingerprints.\n",
    "        \n",
    "        inSDF  = 'ref.sdf'     ## Database SD file\n",
    "        radius = 4             ## Define morgan fingerprint radius\n",
    "\n",
    "    '''\n",
    "    \n",
    "    ### Model Analysis: Loading SDF or given pandasDF, calculating Morgan Fingerprints\n",
    "    \n",
    "    mfpDF = PandasTools.LoadSDF(inSDF, molColName='molRO') ## Dataframe with all Model information\n",
    "    mfpDF['MorganFP']= [ AllChem.GetMorganFingerprintAsBitVect( mol, int(radius)) for mol in mfpDF.molRO]\n",
    "    mfpDF['molID'] = [str('mol%0.6d'%(int(x)+1)) for x in range(len(mfpDF))] ## adding ID column e.g. mol000001\n",
    "\n",
    "    return (mfpDF)\n",
    "\n",
    "def getSimilarity(refDF,mdDF, RFname, MDname, refID, mdID, cutoff, maxsim,  molcol='molRO'):\n",
    "\n",
    "    '''\n",
    "\n",
    "    This function reads two pandas Dataframe where:\n",
    "        - one contains a Reference Database\n",
    "        - and the other one is the Model database\n",
    "\n",
    "    Then a similiraty analysis using Morgan Fingerprints is performed at the cutoff provided\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    refDF: pd.DataFrame \n",
    "        referenceDF ## Reference Dataframe including MorgaFP column previously calculated\n",
    "    mdDF: pd.DataFrame\n",
    "        modelDF ## Model Dataframe including MorgaFP column previously calculated\n",
    "    RFname: str\n",
    "        'Tox21' ## Reference database basename to name files\n",
    "    MDname: str\n",
    "        'myModel' ## Model database basename to name files\n",
    "    refID: str\n",
    "        'ToxCast_chid' ## Reference ID name column\n",
    "    mdID: str\n",
    "        'molID' ## Model ID name column\n",
    "        mdDF['molID'] = [str('mol%0.6d'%(int(x)+1)) for x in range(len(mdDF))]\n",
    "    cutoff: float\n",
    "        0.6 ## cutoff similarity distance by tanimoto Morgan fingerprints\n",
    "    maxsim: float\n",
    "        1   ## maximum similarity cutoff\n",
    "    molcol: str\n",
    "        'mol3Dprot' ## molecule column name which contains rdkit mol object\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    pd.DataFrame\n",
    "        Principal Component Analysis (PCA) DataFrame\n",
    "    SD\n",
    "        SD file containg the whole information with similarity molecules\n",
    "    pickle\n",
    "        TSV file in tabular format\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    \n",
    "    Run first morgan finger prints calculation:\n",
    "    \n",
    "    degdf['MorganFP']= [ AllChem.GetMorganFingerprintAsBitVect( mol, int(radius)) for mol in degdf.mol3DProt]\n",
    "    mulDF['MorganFP']= [ AllChem.GetMorganFingerprintAsBitVect( mol, int(radius)) for mol in mulDF.mol3DProt]\n",
    "    \n",
    "    Define input parameters tu run similarity analisys:\n",
    "    \n",
    "    refD = mulDF.copy()\n",
    "    mdD = degdf.copy()\n",
    "    RFname = 'Mulliner'\n",
    "    MDname = 'Degeneration'\n",
    "    refID = 'parent_nonstd_inkey'\n",
    "    mdID = 'parent_nonstd_inkey'         \n",
    "    cutoff = 1.0           \n",
    "    maxsim = 1.0\n",
    "    molcol = 'mol3DProt'\n",
    "    \n",
    "    simDF = getSimilarity(refDF, mdDF, RFname, MDname, refID, mdID, cutoff, maxsim, molcol)\n",
    "\n",
    "    '''\n",
    "     \n",
    "    ## Loop to find similarity between two databases\n",
    "    \n",
    "    counter = 0\n",
    "    simDF = pd.DataFrame([])\n",
    "    \n",
    "    refIDout = refID\n",
    "    mdIDout = mdID\n",
    "    if refID == mdID:\n",
    "        refIDout += '_ref'\n",
    "        mdIDout += '_model'\n",
    "\n",
    "    for i, row_i in refDF.iterrows():       \n",
    "        max_similarity = 0\n",
    "        max_similarity_id = ''\n",
    "        try:\n",
    "            for j, row_j in mdDF.iterrows():\n",
    "                sim = DataStructs.FingerprintSimilarity(row_j.MorganFP,row_i.MorganFP)\n",
    "                if sim>max_similarity:\n",
    "                    max_similarity = sim\n",
    "                    max_similarity_mol = row_j[molcol]\n",
    "                    max_similarity_id = row_j[mdID]\n",
    "        except:\n",
    "            max_similarity = 0\n",
    "            max_similarity_id= ''\n",
    "\n",
    "        if max_similarity >= cutoff and max_similarity <= maxsim:\n",
    "            counter = counter+1\n",
    "            simDict = [{refIDout:row_i[refID],mdIDout:max_similarity_id,\n",
    "                        'similarity':float(max_similarity),RFname+'_mol': row_i[molcol],\n",
    "                        MDname+'_mol':max_similarity_mol}]\n",
    "            tempDF = pd.DataFrame(simDict)\n",
    "            tempDF = tempDF[[refIDout, mdIDout, 'similarity', RFname+'_mol',MDname+'_mol']]\n",
    "            simDF = simDF.append(tempDF)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    print ('Total similar molecules found = ', counter)    \n",
    "    \n",
    "    if counter > 0:\n",
    "        simDF = simDF.sort_values([mdIDout, 'similarity'], ascending=[True, False])\n",
    "        simDF = simDF.reset_index(drop=True)\n",
    "#         simDF.to_csv('similarityAnalysis_'+RFname+'_vs_'+MDname+'_morganFP.csv',\n",
    "#                      sep='\\t', encoding='utf-8', index=False )\n",
    "        simDF.to_pickle('similarityAnalysis_'+RFname+'_vs_'+MDname+'_morganFP.pkl')\n",
    "\n",
    "        writeSDFfromPandasDF(simDF,'similarityAnalysis_'+RFname+'_vs_'+MDname+'_morganFP.sdf',\n",
    "                             RFname+'_mol',list(simDF.columns))\n",
    "    \n",
    "    return (simDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Similarity by SMARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smartsFinding(DF,molDFname,ID, smarts):\n",
    "\n",
    "    '''\n",
    "        This functions allows one to perfom a searching by Substructure using SMARTS from a Dataframe\n",
    "        \n",
    "        DF = DF                ## Dataframe with molecules\n",
    "        molDFname  = 'molCol'  ## molecule column \n",
    "        ID = 'name'            ## ID or name column\n",
    "        smarts = '[NX3;H2,H1;!$(NC=O);!$(Nc)]' # aliphatic primary and secondary amines but no aniline nor amides\n",
    "           or\n",
    "        smarts = '[NX4;H0]'  # cuaternary amines (charged)\n",
    "        \n",
    "        smartsDF = smartsFinding(DF, molDFname,smarts)\n",
    "        \n",
    "        If one wants an SDFile, use writeSDFfromPandasDF(df, output, molColName, props) function\n",
    "    '''\n",
    "\n",
    "    target = Chem.MolFromSmarts(str(smarts))\n",
    "    DF = DF.reset_index(drop=True)\n",
    "    smartsDF = pd.DataFrame([]) ## Dataframe with all information DF plus matchDF\n",
    "    matchDF = pd.DataFrame([]) ## Dataframe with matching information\n",
    "    count = 0\n",
    "   \n",
    "    for i in range(len(DF)):\n",
    "        mol = DF[molDFname][i]\n",
    "        if mol.HasSubstructMatch(target):\n",
    "            count += 1\n",
    "            temp1 = pd.DataFrame(DF.loc[i,:]).T ## to convert row into Dataframe\n",
    "            smartsDF = smartsDF.append(temp1)\n",
    "            temp2 = {'HasSubMatch':'YES'}\n",
    "            temp2 = pd.DataFrame([temp2])\n",
    "            matchDF = matchDF.append(temp2)\n",
    "        else:\n",
    "            pass\n",
    "#             temp1 = pd.DataFrame(DF.loc[i,:]).T ## to convert row into Dataframe\n",
    "#             smartsDF = smartsDF.append(temp1)\n",
    "#             temp2 = {'HasSubMatch':'NO'}\n",
    "#             temp2 = pd.DataFrame([temp2])\n",
    "#             matchDF = matchDF.append(temp2)\n",
    "            \n",
    "\n",
    "    print ('Total match molecules: ',count)\n",
    "    smartsDF = smartsDF.reset_index(drop=True)\n",
    "    matchDF = matchDF.reset_index(drop=True)\n",
    "    smartsDF = smartsDF.join(matchDF)\n",
    "#     smartsDF = smartsDF.sort_values(['HasSubMatch',ID], ascending=[False,True])\n",
    "    \n",
    "    return (smartsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. etoxLAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. creating automatically a new model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def creatingNewModel(endpoint, descriptor, tag):\n",
    "    '''\n",
    "    This functions allows one to use etoxlab and create a new model automatically\n",
    "        input:\n",
    "            endpoint = 'ratDeG' ## endpoint name\n",
    "            descriptor = 'adriana' ## descriptor name\n",
    "            tag = 'degeneration' ## tag information\n",
    "        \n",
    "        e.g. creatingNewModel('ratDeG', 'adriana','degeneration')\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    os.system('~/soft/eTOXlab/src/manage.py --new -e '+endpoint+' -t /toxicity/'+descriptor+'/'+tag+'/1')\n",
    "\n",
    "    print ('Tag is set up')\n",
    "\n",
    "    os.system('~/soft/eTOXlab/src/manage.py -v 0 -e '+endpoint+' --get=model')\n",
    "    os.system('mv imodel.py '+descriptor+'_imodel.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. RandomSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomsplit(df, dirModelname, activity, molcol, name, endpoint,category, vpath):\n",
    "    '''\n",
    "        df = df                   ## pandas dataframe\n",
    "        dirModelname = '5-model'  ## folder name to be created to store model files\n",
    "        activity = 'activity'     ## columns with activity values\n",
    "        molcol = 'mol3Dprot'      ## molecule column\n",
    "        name = 'parent_inkey'     ## column name\n",
    "        endpoint = 'BioTF'        ## endpoint name for basename files.sdf\n",
    "        category = 'activity'     ## column name to create categories\n",
    "        vpath = os.getcwd()       ## current directory path\n",
    "        \n",
    "        This functions returns 3 pandas dataframes, test, train and global DF\n",
    "        The split is 70/30 train/test, with a randomseed = 1987\n",
    "        It creates 3 SDFiles, global, test and train with files name, activity inside, \n",
    "        ready to build a model. \n",
    "        \n",
    "        e.g. trainDF, testDF, globalDF = (df, dirModelname, activity, molcol, name, endpoint, category, vpath)\n",
    "    '''\n",
    "    \n",
    "\n",
    "    \n",
    "    ## Model Building preparation inputs parameters\n",
    "   \n",
    "    print ('\\033[1m' + '\\n Writing Model SDFile with less fields and Randomsplit'+'\\033[0m')\n",
    "    dirModelname = dirModelname ## name directory to save 3D sdFile with 3D coordinates\n",
    "    createDir(vpath, dirModelname)\n",
    "     \n",
    "    ## Writing model Global SDfile\n",
    "    globaldf = df.copy()\n",
    "    globaldf = globaldf[[name, activity, molcol]]\n",
    "    globaldf.columns = ['name', 'activity', 'mol']\n",
    "    props = list(globaldf.columns)\n",
    "    \n",
    "    outModelfile =  vpath+'/'+dirModelname+'/global-'+endpoint+'.sdf'\n",
    "    writeSDFfromPandasDF(globaldf, outModelfile, 'mol', props)\n",
    "    \n",
    "    ## Writing model training and test set SDfile:\n",
    "\n",
    "    valdf = df[[name, activity, molcol]]\n",
    "    valdf.columns = ['name', 'activity', 'mol']\n",
    "    \n",
    "    train, test, y_train, y_test = train_test_split(valdf, df[category], test_size=0.2, \n",
    "                                                    random_state= 1987, stratify=df[category])\n",
    "    \n",
    "    train\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    \n",
    "    trainfile = vpath+'/'+dirModelname+'/tr-'+endpoint+'.sdf'\n",
    "    testfile = vpath+'/'+dirModelname+'/pr-'+endpoint+'.sdf'\n",
    "    \n",
    "    trainprops = list(train.columns)\n",
    "    testprops = list(test.columns)\n",
    "    \n",
    "    writeSDFfromPandasDF(train, trainfile, 'mol', trainprops)\n",
    "    writeSDFfromPandasDF(test, testfile, 'mol', testprops)\n",
    "    print ('Test : ' + str(len(test)))\n",
    "    print ('Train : ' + str(len(train)))\n",
    "    print ('Global : ' + str(len(df)))\n",
    "    \n",
    "    return ( train, test, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomsplit_quantitative(df, dirModelname, activity, molcol, name, endpoint, catcol, vpath):\n",
    "    '''\n",
    "        df = df                   ## pandas dataframe\n",
    "        dirModelname = '5-model'  ## folder name to be created to store model files\n",
    "        molcol = 'mol3Dprot'      ## molecule column\n",
    "        name = 'parent_inkey'     ## column name\n",
    "        endpoint = 'BioTF'        ## endpoint name for basename files.sdf \n",
    "        vpath = os.getcwd()       ## current directory path\n",
    "        \n",
    "        This functions returns 3 pandas dataframes, test, train and global DF\n",
    "        The split is 70/30 train/test, with a randomseed = 1987\n",
    "        It creates 3 SDFiles, global, test and train with files name, activity inside, \n",
    "        ready to build a model. \n",
    "        \n",
    "        e.g. trainDF, testDF, globalDF = (df, dirModelname, activity, molcol, name, endpoint, vpath)\n",
    "    '''\n",
    "    df = df.copy()\n",
    "#     df = df[[name, activity, molcol]]\n",
    "#     df.columns = ['name', 'activity', 'mol']\n",
    "    \n",
    "    ## Model Building preparation inputs parameters\n",
    "   \n",
    "    print ('\\033[1m' + '\\n Writing Model SDFile with less fields and Randomsplit'+'\\033[0m')\n",
    "    dirModelname = dirModelname ## name directory to save 3D sdFile with 3D coordinates\n",
    "    createDir(vpath, dirModelname)\n",
    "     \n",
    "    ## Writing model Global SDfile\n",
    "    props = list(df.columns)\n",
    "    \n",
    "    outModelfile =  vpath+'/'+dirModelname+'/global-'+endpoint+'.sdf'\n",
    "    writeSDFfromPandasDF(df, outModelfile, molcol, props)\n",
    "    \n",
    "    ## Writing model training and test set SDfile:\n",
    "    train, test, y_train, y_test = train_test_split(df,df[activity], test_size=0.2, \n",
    "                                                    random_state= 1987, stratify=df[catcol])\n",
    "    \n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    \n",
    "    trainfile = vpath+'/'+dirModelname+'/tr-'+endpoint+'.sdf'\n",
    "    testfile = vpath+'/'+dirModelname+'/pr-'+endpoint+'.sdf'\n",
    "    \n",
    "    trainprops = list(train.columns)\n",
    "    testprops = list(test.columns)\n",
    "    \n",
    "    writeSDFfromPandasDF(train, trainfile, molcol, trainprops)\n",
    "    writeSDFfromPandasDF(test, testfile, molcol, testprops)\n",
    "    print ('Test : ' + str(len(test)))\n",
    "    print ('Train : ' + str(len(train)))\n",
    "    print ('Global : ' + str(len(df)))\n",
    "    \n",
    "    return ( train, test, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitSDF (sdf, prop, seed):\n",
    "    \n",
    "    '''\n",
    "        This function allows one to obtain a training and test set by\n",
    "        randomspliting. \n",
    "        \n",
    "        Input paremeters:\n",
    "            sdf = file.sdf ### sd File input pathway\n",
    "            prop = 70      ### give the percentage of splitting ( 70% trainset, 30% testset)\n",
    "            seed = 483     ### give a random number \n",
    "        \n",
    "        \n",
    "        \n",
    "        e.g. randomSplitSDF -f file.sdf -p 70 [-s 2356]\n",
    "    '''\n",
    "    prop = float(prop)\n",
    "    seed = float(seed)\n",
    "\n",
    "    nmols = 0\n",
    "    try:\n",
    "        f = open (sdf,'r')\n",
    "    except:\n",
    "        print ('unable to open file ',sdf, ' ABORT')\n",
    "        exit(1)\n",
    "        \n",
    "    for line in f:\n",
    "        if line.startswith('$$$$'):\n",
    "            nmols = nmols+1\n",
    "    f.close()\n",
    "\n",
    "    ntrai = int(np.round(prop*nmols/100.0))\n",
    "    npred = nmols - ntrai\n",
    "    \n",
    "    print (nmols, \"compounds found. Creating series of \", ntrai, \" for training and \", npred, \" for prediction\")\n",
    "\n",
    "    if seed != None :\n",
    "        npseed = int(seed)\n",
    "        np.random.seed(npseed)\n",
    "        \n",
    "    elements = np.random.choice(nmols, ntrai, False)\n",
    "    #print elements\n",
    "    \n",
    "    f  = open (sdf,'r')\n",
    "    sdf = sdf.split(\"/\")\n",
    "    sdf = sdf[-1]\n",
    "    fp = open ('pr-'+sdf,'w')\n",
    "    ft = open ('tr-'+sdf,'w')\n",
    "\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        if i in elements :\n",
    "            ft.write(line)\n",
    "        else:\n",
    "            fp.write(line)\n",
    "        if line.startswith('$$$$'):\n",
    "            i=i+1\n",
    "\n",
    "    f.close()\n",
    "    fp.close()\n",
    "    ft.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
